# 다층 퍼셉트론

## 신경망 기초

### 신경망의 종류
* 전방신경망(feedforward)과 순환신경망(recurrent)
  * 전방신경망은 모든 계산이 왼쪽에서 오른쪽으로 진행된다. 순환 신경망은 오른쪽에서 왼쪽으로 진행하는 피드백 계산도 포함한다.
* 얕은 신경망과 깊은 신경망
  * 은닉층이 1~2개 정도인 신경망을 얕은 신경망이라고 하며, 더 많은 은닉층을 가진 신경망을 깊은 신경망이라고 한다.
  * 몇개의 은닉층을 기준으로 구분하는지는 명확하지 않다.
* 결정론(deterministic)신경망과 스토캐스틱(stochastic)신경망
  * 결정론 신경망은 입력이 같으면 항상 같은 출력이 나오며, 계산식에 임의성이 전혀 없다.
  * 스토캐스틱 신경망에서는 계산식이 확률에 따른 난수를 사용하므로 입력이 같아도 매번 다른 출력이 나온다.

## 퍼셉트론
* 주어진 데이터가 선형 분리할 수 있다면 미분을 활용한 학습 알고리즘은 반드시 100%의 정확률로 수렴한다는 정리가 증명되었다.
* 딥러닝을 포함하여 현대 신경망은 퍼셉트론을 병렬 구조와 순차 구조로 결합한 형태이다.

### 구조
<페셉트로 이미지>
* 입력층에 있는 노드 하나는 특징 벡터의 특징 하나에 해당한다. 
* 따라서 특징 벡터를 **x**=(x<sub>1</sub>, x<sub>2</sub>, ... ,x<sub>d</sub>)<sup>T</sup>로 표기한다면 입력층은 d개의 노드를 가진다.
* 맨 위에 x<sub>0</sub>로 표시된 것은 바이어스라고 부르며, 입력 노드의 총 개수는 d+1개이다.
* 출력층은 y라 표기된 하나의 노드를 가진다.
* 입력노드는 모두 출력 노드와 에지로 연결되어 있는데, 에지는 w<sub>i</sub>로 표기된 가중치(weight)를 가진다.(d+1개의 가중치가 있다.)

### 동작
<수학식 138p>
<행렬 수학식 139p>
* 입력층에 특징벡터 **x**가 들어오면 서로 연결된 특징값과 가중치를 곱해서 더한다.
* 이렇게 얻은 s를 활성함수(Activation function)에 입력으로 넣고 계산한다.
* 활성함수의 출력이 퍼셉트론의 최종 출력이 되는데, 1또는 -1이다, 

### 분류기로 해석
<분류기 식 140p>
* 위 식은 직선의 방정식으로 w<sub>1</sub>와 w<sub>2</sub>는 직선의 방향, w<sub>0</sub>는 절편에 해당한다.
* 일반적으로 d차원의 공간에서 w<sub>1</sub> ~ w<sub>d</sub>는 초평면의 방향, w<sub>0</sub>는 절편을 나타낸다.
* w<sub>0</sub>는 특징이 모두 0인 신호가 들어올 때 퍼셉트론이 출력할 기본값으로 퍼셉트론이 0으로 부터 벗어난 정도를 나타내므로 바이어스라고 부른다.

<분류기 식 140p>

* 만약 퍼셉트론이 2개의 입력을 받는다면 해당 퍼셉트론은 2차원 특징 공간을 +와 -영역으로 나눈며, +영역의 모든 점은 +1로 -영역의 모든 점은 -1로 변환한다.
* 따라서 퍼셉트론은 이진 분류기로 동작한다.
* 이쳐럼 특징 공간을 2개의 영역으로 나눔으로써 패턴의 부류를 결정하는 경계를 결정 경계(decision boundary)라고 한다.
* 퍼셉트론은 선형 방정식을 사용하므로, 2차원에선 결정 경계가 직선이 되는데 이 직선을 결정 직선(decisoin line)이라고 한다.
* 3차원에서는 공간을 둘로 나누는 평면이 되어 결정 평면(decision plane)이라고 한다.
* 4차원 이상에서는 결정 초평면(decision hyperplane)이라고 한다.
* 위와 같은 사실이 모두 퍼셉트론이 선형 분류기(linear classifier)라는 사실을 나타낸다.

### 학습
* 기계 학습에서 추정해야하는 매개변수 &theta;는 가중치 **w** = (w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>d</sub>)<sup>T</sup>를 알아내야 하므로 **w**가 &theta;이다.
* 목적함수는 J(&theta;)또는 J(**w**)로 나타내며 목적함수의 조건은 다음과 같다.
  * J(**w**) &ge; 0이다.
  * **w**가 최적이면, 즉 모든 샘플을 맞히면 J(**w**) = 0 이다.
  * 틀리는 샘플이 많은 **w**일수록 J(**w**)는 큰 값을 갖는다.
<142p>
* 위 식에서 가중치 갱신 규칙, &theta; = &theta; - &rho;g를 적용한다.
* 여기서 그래디언트 g를 얻기위해 J(**w**)를 w<sub>i</sub>로 편미분한다.

<편미분 식 143p>

* 위 식을 가중치 갱신 규칙에 넣으면 다음과 같은 식이 되고 그래디언트는 목적한수의 값이 커지는 방향이므로 -를 붙여 더한다.
* &rho;는 학습률이다.
* 따라서 다음 식을 퍼셉트론 학습 규칙으로서 보통 델타 규칙(delta rule)이라고 한다.

<델타 규칙 143p>

* 퍼셉트론 학습 알고리즘은 훈련집합이 선형 분리될 수 있다면, 반드시 수렴한다는 사실이 증명 되어 있다. 하지만 선형분리가 불가능한 훈련집합에 대해서는 무한 반복이다.

<델타 규칙 행렬 145p>

## 다층 퍼셉트론

* 다층 퍼셉트론 : 여러개의 퍼셉트론을 결합한 다층 구조를 이용하여 선형 분리가 불가능한 상황을 해결한다.
  * 은닉층 : 원래 특징 공간을 분류하는데 훨씬 유리한 새로운 특징공간으로 변환한다.
  * 시그모이드 활성화 함수 : 퍼셉트론은 계단 함수를 활성 함수로 사용하는데 이 함수는 경성(hard)의사결정에 해당한다. 다층 퍼셉트론은 연성(soft)의사 결정이 가능한 시그모이드 활성화 함수를 사용한다. 연성에서는 출력이 연속값인데, 출력을 신뢰도로 간주함으로써 더 융통성 있게 의사 결정을 할 수 있다.
  * 오류 역전파 알고리즘 : 역방향으로 진행하면서 한 번에 한 층씩 그레이디언트를 계산하고 가중치를 갱산하는 방식의 오류 역전파 알고리즘을 사용한디.

### 특징 공간의 변환
* 퍼셉트론 2개가 있다면, 결정 직선 2개를 이용하여 특징 공간을 A, B, C 3 가지 공간으로, 즉 부분공간 3개로 나눌 수 있다.
* 퍼셉트론 2개가 병렬로 연결되어 2개의 출력이 나온다면,원래 특징 공간 **x** = (x<sub>1</sub>, x<sub>2</sub>)<sup>T</sup>를 새로운 특징공간 **z** = (z<sub>1</sub>, z<sub>2</sub>)<sup>T</sup>로 변환한다.
* 2개의 퍼셉트론이 병렬로, 다음 층에 출력이 1개인 퍼셉트론이 연결된 구조는 2개의 병렬 퍼셉트론은 새로운 공간으로 특징들을 변환하고 마지막 출력층의 퍼셉트론이 새롭게 변환된 공간을 나눈다.
* 다층 퍼셉트론에서 영역이 하나의 점으로 매핑되는 것은 계단 함수를 활성화 함수로 사용했기 때문이다.
* 원래 특징공간이 2차원이고 퍼셉트론 p개 결합한 상황으로 일반화해 보자면, 이때 신경망은 2차원 공간에서 p차원 공간으로 변환된다.
* p가 클수록 신경망의 용량은 크며, 용량이 너무 커지면 과잉적합될 가능성이 커진다.

### 활성화 함수
* 로지스틱 시그모이드를 사용하는 퍼셉트론은 (-1,1)사이의 실수 출력값을 활률 또는 신뢰도로 해석하여 더 정확한 의사결정 또는 추가적인 추론 등에 활용한다.
* 딥러닝이 로직스틱이나 하이퍼볼릭 탄젠트를 사용하면 그래디언트 소멸(Gradient Vanishing)문제가 발생할 가능성이 있다.
* 이떄 렉티 파이어 함수로 대치하면 문제가 크게 완화된다. 또한 렉티파이어의 그래디언트는 비교 연산 한 번으로 계산할 수 있어 답러닝의 속도 향상에 무척 유용하다. 하지만 음수를 모두 0으로 대치하기 때문에 문제가 발생한다.

### 구조
* 입력층과 출력층 사이에 놓인 새로운 층을 은닉층(hidden layer)라고 한다.
* 퍼셉트론과 달리 층이 여러 개이기 때문에 다층 퍼셉트론(Multi Layer Perceptron: MLP)라고 한다.
* 은닉층이 대략 4개 이상 되면 깊은 신경망으로 취급하며, 깊은 신경망을 학습시키는 알고리즘을 딥러닝이라고 한다.
* 훈련집합이 주어져 문제가 확정되면 다층 퍼셉트론의 구조 중 입력 노드와 출력 노드의 개수는 자동으로 결정된다.
* 은닉층에 있는 노드의 갸수 p는 가용자가 지정하는 하이퍼 매개변수이다. p는 다층 퍼셉트론의 용량을 규종한다 너무 크면 과잉적합, 너무 적으면 과소적합의 위험이 있다.
* 2층 퍼셉트론에서 매개변수는 입력층과 은닉층, 은닉층과 출력층을 연결하는 U<sup>1</sup>과 U<sup>2</sup>로 구분하여 표기한다.
* U<sup>l</sup><sub>ji</sub>는 l-1번째 은닉층의 i번째 노드와 j번째 노드를 연결하는 가중치라는 뜻이다.

#### 병렬분산 구조
* 